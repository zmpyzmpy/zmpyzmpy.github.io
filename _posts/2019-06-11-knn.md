---
layout: post
title:  "K nearest neighbors"
date: 2019-05-23 17:02:24 +0800
tags: Machine Learning
---

\section{K nearest neighbors}

\noindent Given a training dataset and a new instance, find the new instance's k closest instances in training set, then classify the new instance the class that majority of the k instances belong to.

\noindent More generally, given training set
$$
	T = \{ (x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}
$$

\noindent where $x_i \in \mathcal{X}\subseteq\mathbf{R}^n$ is the feature vector of an instance, $y_i\in\mathcal{Y}=\{c_1,c_2,\cdots,c_K\}$ is the class of an instance, $i=1,2,\cdots,N$. Input a new instance's feature vector $x$, output $x$'s class $y$.
\begin{enumerate}
​	\item Based on given distance measure, find $x$'s k nearest points in $T$, the region that covers these k points is denoted as $N_k(x)$.
​	\item Decide $x$'s class $y$ based on certain classification rule (such as majority vote) in $N_k(x)$.
\end{enumerate}

\noindent This whole procedure can be abstracted as
$$
	y = \arg\max_{c_j}\sum_{x_i\in N_k(x)} I(y_i==c_i), i=1,2,\cdots,N;j=1,2,\cdots,K
$$

\section{Distance measurement metrics}

Suppose the feature space $\mathcal{X}$ is n dimensional real number space $\mathbf{R}^n$. For two vectors $x_i, x_j\in \mathcal{X}$, $x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$, $x_j=(x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)})^T$. The $L_p$ distance between $x_i$ and $x_j$ is defined as

$$
	L_p(x_i, x_j) = \Big(\sum_{l=1}^n |x_i^{(l)} - x_j^{(l)}|^p\Big)^{\frac{1}{p}}
$$

\noindent where $p\geq 1$. When $p=2$, the distance is called Euclidean distance, defined as

$$
	L_2(x_i, x_j) = \Big(\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|\Big)^{\frac{1}{2}}
$$

\noindent When $p=1$, the distance is called Euclidean distance, defined as

$$
	L_1(x_i, x_j) = \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|
$$

\noindent When $p=\infty$, it is the maximum distance between each coordinates, defined as

$$
L_\infty(x_i, x_j) = \max_{l}|x_i^{(l)}-x_j^{(l)}|
$$

